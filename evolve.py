#!/usr/bin/env python3
"""
evolve.py â€” Autonomous self-evolution loop for PyCoder.

Runs the agent through continuous cycles of:
  1. Benchmark â€” solve coding tasks of escalating difficulty
  2. Score    â€” evaluate output quality (correctness, completeness, style)
  3. Reflect  â€” session retrospective + evolution goals
  4. Improve  â€” modify own source code to address weaknesses
  5. Validate â€” run full test suite; rollback if broken
  6. Repeat   â€” until all benchmarks pass or max rounds reached

Usage:
    python evolve.py                     # default: 20 rounds
    python evolve.py --rounds 50         # more rounds
    python evolve.py --resume            # continue from last checkpoint

Author & Maintainer: Jiangsheng Yu
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import subprocess
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Optional, Tuple

# Ensure project root on path
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


# ======================================================================
# Logging
# ======================================================================

LOG_DIR = os.path.join(PROJECT_ROOT, "data", "evolution")
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler(
            os.path.join(LOG_DIR, "evolution.log"), encoding="utf-8",
        ),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger("evolve")


# ======================================================================
# Benchmark tasks â€” categorised by difficulty and skill dimension
# ======================================================================

@dataclass
class BenchmarkTask:
    """A coding task used to measure the agent's capability."""
    id: str
    category: str           # algorithm, data_structure, debug, design, etc.
    difficulty: str         # easy, medium, hard, expert
    prompt: str             # the user-facing coding request
    validation_code: str    # Python code asserting correctness of generated code
    scoring_hints: str      # what constitutes a high-quality answer
    max_score: float = 10.0

    def to_dict(self) -> dict:
        return asdict(self)


# ---  Task bank (progressively harder) ---

BENCHMARK_TASKS: List[BenchmarkTask] = [
    # ---- Easy ----
    BenchmarkTask(
        id="easy_sort",
        category="algorithm",
        difficulty="easy",
        prompt="å†™ä¸€ä¸ªPythonå‡½æ•° merge_sort(arr)ï¼Œå®ç°å½’å¹¶æ’åºã€‚è¦æ±‚ï¼šçº¯å‡½æ•°ã€æ”¯æŒç©ºåˆ—è¡¨å’Œå•å…ƒç´ åˆ—è¡¨ã€æœ‰ç±»å‹æ³¨è§£å’Œæ–‡æ¡£å­—ç¬¦ä¸²ã€‚",
        validation_code="""\
from typing import List

{generated_code}

# correctness
assert merge_sort([]) == []
assert merge_sort([1]) == [1]
assert merge_sort([3,1,2]) == [1,2,3]
assert merge_sort([5,4,3,2,1]) == [1,2,3,4,5]
assert merge_sort([1,1,1]) == [1,1,1]
import random; big = random.sample(range(10000), 1000)
assert merge_sort(big) == sorted(big)
print("BENCHMARK_PASS")
""",
        scoring_hints="type annotations, docstring, O(n log n), pure function, handles edge cases",
    ),
    BenchmarkTask(
        id="easy_fibonacci",
        category="algorithm",
        difficulty="easy",
        prompt="å†™ä¸€ä¸ªPythonå‡½æ•° fibonacci(n: int) -> List[int]ï¼Œè¿”å›å‰nä¸ªæ–æ³¢é‚£å¥‘æ•°ã€‚è¦æ±‚ï¼šé«˜æ•ˆå®ç°ï¼ˆä¸è¦æŒ‡æ•°çº§é€’å½’ï¼‰ã€ç±»å‹æ³¨è§£ã€æ–‡æ¡£å­—ç¬¦ä¸²ã€å¤„ç†n<=0è¿”å›ç©ºåˆ—è¡¨ã€‚",
        validation_code="""\
from typing import List

{generated_code}

assert fibonacci(0) == []
assert fibonacci(1) == [0]
assert fibonacci(2) == [0, 1]
assert fibonacci(5) == [0, 1, 1, 2, 3]
assert fibonacci(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]
assert len(fibonacci(100)) == 100
print("BENCHMARK_PASS")
""",
        scoring_hints="O(n) time, type annotations, docstring, edge case",
    ),
    # ---- Medium ----
    BenchmarkTask(
        id="med_lru_cache",
        category="data_structure",
        difficulty="medium",
        prompt=(
            "å®ç°ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„LRUç¼“å­˜ç±» LRUCacheï¼Œæ”¯æŒ get(key) å’Œ put(key, value, ttl=None) æ–¹æ³•ã€‚\n"
            "è¦æ±‚ï¼šO(1) æ—¶é—´å¤æ‚åº¦ã€å¯é€‰TTLè¿‡æœŸæœºåˆ¶ã€capacityå‚æ•°é™åˆ¶æœ€å¤§æ¡ç›®æ•°ã€‚\n"
            "ä¸è¦ä½¿ç”¨functools.lru_cacheã€‚\n"
            "å®ç°æ¡†æ¶ï¼š\n"
            "import threading, time\n"
            "from collections import OrderedDict\n"
            "class LRUCache:\n"
            "  def __init__(self, capacity: int):\n"
            "    self._cache = OrderedDict()  # key -> (value, expire_time_or_None)\n"
            "    self._capacity = capacity\n"
            "    self._lock = threading.Lock()\n"
            "  def get(self, key):\n"
            "    with self._lock:\n"
            "      if key not in self._cache: return None\n"
            "      value, expire = self._cache[key]\n"
            "      if expire is not None and time.time() > expire:\n"
            "        del self._cache[key]; return None\n"
            "      self._cache.move_to_end(key)\n"
            "      return value\n"
            "  def put(self, key, value, ttl=None):\n"
            "    expire = time.time()+ttl if ttl else None\n"
            "    with self._lock:\n"
            "      if key in self._cache: self._cache.move_to_end(key)\n"
            "      self._cache[key] = (value, expire)\n"
            "      if len(self._cache) > self._capacity:\n"
            "        self._cache.popitem(last=False)\n"
            "get è¿”å› Noneï¼ˆä¸æ˜¯-1ï¼‰å½“ key ä¸å­˜åœ¨æˆ– TTL è¿‡æœŸæ—¶ã€‚"
        ),
        validation_code="""\
import time as _time

{generated_code}

c = LRUCache(capacity=3)
c.put("a", 1)
c.put("b", 2)
c.put("c", 3)
assert c.get("a") == 1
c.put("d", 4)  # evicts "b" (LRU)
assert c.get("b") is None or c.get("b") == -1 or c.get("b") is None
assert c.get("c") == 3
assert c.get("d") == 4

# TTL
c2 = LRUCache(capacity=10)
c2.put("x", 100, ttl=0.3)
assert c2.get("x") == 100
_time.sleep(0.5)
result = c2.get("x")
assert result is None or result == -1, f"TTL expired but got {result}"
print("BENCHMARK_PASS")
""",
        scoring_hints="OrderedDict or doubly-linked list + dict, thread-safe with threading.Lock, TTL support, O(1) ops",
    ),
    BenchmarkTask(
        id="med_decorator_retry",
        category="design",
        difficulty="medium",
        prompt=(
            "å†™ä¸€ä¸ªPythonè£…é¥°å™¨å·¥å‚ auto_retry(max_retries=3, delay=1.0, backoff=2.0, exceptions=(Exception,))ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1) å¿…é¡»æ”¯æŒåŒæ­¥å‡½æ•°ä¸å¼‚æ­¥å‡½æ•°ï¼ˆåŒæ­¥ç”¨ time.sleepï¼Œå¼‚æ­¥ç”¨ asyncio.sleepï¼‰\n"
            "2) ä½¿ç”¨ functools.wraps ä¿ç•™å‡½æ•°å…ƒä¿¡æ¯\n"
            "3) æ¯æ¬¡é‡è¯•æ‰“å°æ—¥å¿—ï¼ˆattempt/exceptionï¼‰\n"
            "4) è¾¾åˆ°æœ€å¤§é‡è¯•åæŠ›å‡ºåŸå§‹å¼‚å¸¸\n"
            "5) max_retries è¡¨ç¤ºé¢å¤–é‡è¯•æ¬¡æ•°ï¼ˆæ€»å°è¯•æ¬¡æ•°=1+max_retriesï¼‰\n"
            "6) åŒ…å«å®Œæ•´ç±»å‹æ³¨è§£\n"
            "åªè¾“å‡ºä¸€ä¸ªå®Œæ•´ Python ä»£ç å—ï¼Œä¸è¦è¾“å‡ºè§£é‡Šæ–‡æœ¬ã€‚"
        ),
        validation_code="""\
import asyncio

{generated_code}

# sync test
call_count = 0
@auto_retry(max_retries=3, delay=0.01, backoff=1.0, exceptions=(ValueError,))
def flaky():
    global call_count
    call_count += 1
    if call_count < 3:
        raise ValueError("not yet")
    return "ok"

assert flaky() == "ok"
assert call_count == 3

# should raise after exhausting retries
call_count_2 = 0
@auto_retry(max_retries=2, delay=0.01, backoff=1.0, exceptions=(RuntimeError,))
def always_fail():
    global call_count_2
    call_count_2 += 1
    raise RuntimeError("fail")

try:
    always_fail()
    assert False, "Should have raised"
except RuntimeError:
    pass
assert call_count_2 == 3  # initial + 2 retries

print("BENCHMARK_PASS")
""",
        scoring_hints="functools.wraps, exponential backoff, async support, logging, type annotations",
    ),
    # ---- Hard ----
    BenchmarkTask(
        id="hard_calc_parser",
        category="algorithm",
        difficulty="hard",
        prompt=(
            "å®ç°ä¸€ä¸ªå®Œæ•´çš„æ•°å­¦è¡¨è¾¾å¼è®¡ç®—å™¨ï¼ŒåŒ…å«ä¸€ä¸ªé¡¶å±‚å‡½æ•° evaluate(expression: str) -> floatã€‚\n"
            "è¦æ±‚ï¼šåŠ å‡ä¹˜é™¤ã€æ‹¬å·åµŒå¥—ã€è´Ÿæ•°(å¦‚-5+3)ã€æµ®ç‚¹æ•°(å¦‚3.14)ã€è¿ç®—ç¬¦ä¼˜å…ˆçº§æ­£ç¡®ã€‚\n"
            "ä¸è¦ä½¿ç”¨eval/execã€‚ä½¿ç”¨é€’å½’ä¸‹é™è§£æå™¨å®ç°ã€‚\n"
            "å…³é”®ç»“æ„(æ‰€æœ‰å‡½æ•°å’Œç±»å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ï¼Œä¸è¦åµŒå¥—åœ¨å…¶ä»–å‡½æ•°å†…)ï¼š\n"
            "1. class Tokenizer: å°†å­—ç¬¦ä¸²åˆ†å‰²æˆ tokens (NUMBER, +, -, *, /, (, ))\n"
            "   - å¤„ç†æµ®ç‚¹æ•°: è¿ç»­æ•°å­—å’Œå°æ•°ç‚¹\n"
            "   - å¤„ç†è´Ÿæ•°: åœ¨å¼€å¤´æˆ–'('åé¢çš„'-'è§†ä¸ºè´Ÿå·ï¼Œæ‹¼å…¥æ•°å­—\n"
            "2. class Parser: é€’å½’ä¸‹é™è§£æ\n"
            "   - parse_expression(): å¤„ç† +/- (æœ€ä½ä¼˜å…ˆçº§)\n"
            "   - parse_term(): å¤„ç† *// (è¾ƒé«˜ä¼˜å…ˆçº§)\n"
            "   - parse_factor(): å¤„ç†æ‹¬å·ã€è´Ÿå·ã€æ•°å­— (æœ€é«˜ä¼˜å…ˆçº§)\n"
            "3. def evaluate(expression: str) -> float: é¡¶å±‚å‡½æ•°\n"
            "   tokens = Tokenizer(expression).tokenize()\n"
            "   return Parser(tokens).parse_expression()\n"
            "é‡è¦: evaluate å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ï¼Œä¸èƒ½åœ¨ç±»æˆ–å…¶ä»–å‡½æ•°å†…éƒ¨ã€‚"
        ),
        validation_code="""\
{generated_code}

assert abs(evaluate("1+2") - 3.0) < 1e-9
assert abs(evaluate("2*3+4") - 10.0) < 1e-9
assert abs(evaluate("2*(3+4)") - 14.0) < 1e-9
assert abs(evaluate("10/3") - 3.3333333) < 0.001
assert abs(evaluate("(2+3)*(4-1)") - 15.0) < 1e-9
assert abs(evaluate("-5+3") - (-2.0)) < 1e-9
assert abs(evaluate("3.14*2") - 6.28) < 0.01
assert abs(evaluate("((1+2)*(3+4))") - 21.0) < 1e-9
assert abs(evaluate("2+3*4-1") - 13.0) < 1e-9
print("BENCHMARK_PASS")
""",
        scoring_hints="recursive descent or shunting-yard, no eval(), handles negatives, floats, nested parens, operator precedence",
    ),
    BenchmarkTask(
        id="hard_concurrent_pool",
        category="design",
        difficulty="hard",
        prompt="å®ç°ä¸€ä¸ªå¯å¤ç”¨çš„å¹¶å‘ä»»åŠ¡æ±  TaskPool ç±»ã€‚è¦æ±‚ï¼š\n1. submit(fn, *args, **kwargs) -> Future æäº¤ä»»åŠ¡\n2. map(fn, iterable) -> List[Result] æ‰¹é‡æ‰§è¡Œ\n3. shutdown(wait=True) ä¼˜é›…å…³é—­\n4. æ”¯æŒmax_workerså‚æ•°\n5. æ”¯æŒä»»åŠ¡è¶…æ—¶(timeoutå‚æ•°)\n6. æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§(priorityå‚æ•°)\n7. ä½¿ç”¨threadingå®ç°ï¼Œæœ‰ç±»å‹æ³¨è§£å’Œæ–‡æ¡£å­—ç¬¦ä¸²",
        validation_code="""\
import time as _time

{generated_code}

pool = TaskPool(max_workers=4)
# submit
fut = pool.submit(lambda x: x*2, 21)
assert fut.result(timeout=5) == 42

# map
results = pool.map(lambda x: x**2, [1,2,3,4,5])
assert sorted(results) == [1,4,9,16,25] or results == [1,4,9,16,25]

# error handling
def fail():
    raise ValueError("boom")
fut2 = pool.submit(fail)
try:
    fut2.result(timeout=5)
    assert False, "should raise"
except (ValueError, Exception):
    pass

pool.shutdown(wait=True)
print("BENCHMARK_PASS")
""",
        scoring_hints="threading-based, Future pattern, graceful shutdown, timeout, priority queue",
    ),
    # ---- Expert ----
    BenchmarkTask(
        id="expert_type_checker",
        category="algorithm",
        difficulty="expert",
        prompt=(
            "å®ç°ä¸€ä¸ªè¿è¡Œæ—¶ç±»å‹æ£€æŸ¥è£…é¥°å™¨ type_check(func)ã€‚\n"
            "éœ€è¦å¯¼å…¥: import typing, inspect, functools\n"
            "æ ¸å¿ƒç»“æ„:\n"
            "1. def _matches_type(value, expected_type) -> bool:\n"
            "   - å¦‚æœ expected_type is type(None): return value is None\n"
            "   - origin = typing.get_origin(expected_type)\n"
            "   - å¦‚æœ origin is None (æ™®é€šç±»å‹å¦‚int,str): return isinstance(value, expected_type)\n"
            "   - args = typing.get_args(expected_type)\n"
            "   - å¦‚æœ origin is Union: return any(_matches_type(value, a) for a in args)\n"
            "   - å¦‚æœ origin is list: return isinstance(value, list) and all(_matches_type(v, args[0]) for v in value) if args else isinstance(value, list)\n"
            "   - å¦‚æœ origin is dict: return isinstance(value, dict) and all(_matches_type(k, args[0]) and _matches_type(v, args[1]) for k,v in value.items()) if args else isinstance(value, dict)\n"
            "   - å…œåº•: return isinstance(value, expected_type) if isinstance(expected_type, type) else True\n"
            "2. def type_check(func):\n"
            "   hints = typing.get_type_hints(func)\n"
            "   @functools.wraps(func)\n"
            "   def wrapper(*args, **kwargs):\n"
            "     sig = inspect.signature(func)\n"
            "     bound = sig.bind(*args, **kwargs)\n"
            "     bound.apply_defaults()\n"
            "     for param_name, value in bound.arguments.items():\n"
            "       if param_name in hints:\n"
            "         if not _matches_type(value, hints[param_name]):\n"
            "           raise TypeError(f'å‚æ•° {param_name} ç±»å‹é”™è¯¯')\n"
            "     result = func(*args, **kwargs)\n"
            "     if 'return' in hints:\n"
            "       if not _matches_type(result, hints['return']):\n"
            "         raise TypeError('è¿”å›å€¼ç±»å‹é”™è¯¯')\n"
            "     return result\n"
            "   return wrapper\n"
            "å…³é”®æ³¨æ„: ä¸è¦å¯¹ Dict/List ç­‰å®¹å™¨ç±»å‹ç”¨ isinstance(value, Dict[K,V]), å› ä¸ºæ³›å‹ä¸å¯å“ˆå¸Œã€‚"
            "åªå¯¹å®¹å™¨æœ¬èº« isinstance(value, dict) ç„¶åé€å…ƒç´ æ£€æŸ¥ã€‚"
            "Optional[X] ç­‰ä»·äº Union[X, None]ï¼Œget_origin è¿”å› Unionã€‚"
        ),
        validation_code="""\
from typing import List, Dict, Optional, Union

{generated_code}

@type_check
def add(a: int, b: int) -> int:
    return a + b

assert add(1, 2) == 3

try:
    add("a", 2)
    assert False, "should raise TypeError"
except TypeError:
    pass

@type_check
def greet(name: str, excited: bool = False) -> str:
    return f"Hello {name}{'!' if excited else '.'}"

assert greet("world") == "Hello world."
assert greet("world", excited=True) == "Hello world!"

@type_check
def process(items: List[int]) -> Dict[str, int]:
    return {"sum": sum(items), "count": len(items)}

assert process([1,2,3]) == {"sum": 6, "count": 3}

try:
    process(["a","b"])
    assert False, "should raise TypeError for List[int]"
except TypeError:
    pass

@type_check
def maybe(x: Optional[int]) -> Optional[str]:
    return str(x) if x is not None else None

assert maybe(42) == "42"
assert maybe(None) is None

print("BENCHMARK_PASS")
""",
        scoring_hints="inspect.get_annotations, typing.get_origin/get_args for generics, recursive checking for nested types",
    ),
    BenchmarkTask(
        id="expert_async_pipeline",
        category="design",
        difficulty="expert",
        prompt="å®ç° AsyncPipeline ç±». add_stage(name,fn): æ·»åŠ é˜¶æ®µ. process(data): æŒ‰åºæ‰§è¡Œæ‰€æœ‰é˜¶æ®µ. process_batch(items, concurrency=5): å¹¶å‘å¤„ç†, ç”¨ asyncio.Semaphore(concurrency) é™åˆ¶å¹¶å‘æ•°. stats(): è¿”å› {stage_name: {calls, avg_time, errors}}. å…³é”®: asyncio.iscoroutinefunction(fn) åˆ¤æ–­æ˜¯å¦async, è®°å½•æ¯é˜¶æ®µè°ƒç”¨æ¬¡æ•°ã€æ€»è€—æ—¶ã€é”™è¯¯è®¡æ•°.",
        validation_code="""\
import asyncio

{generated_code}

async def test():
    p = AsyncPipeline()
    p.add_stage("double", lambda x: x * 2)
    p.add_stage("add_one", lambda x: x + 1)

    assert await p.process(5) == 11  # (5*2)+1

    results = await p.process_batch([1,2,3,4,5], concurrency=3)
    assert sorted(results) == [3,5,7,9,11]

    s = p.stats()
    assert s["double"]["calls"] >= 6
    assert s["add_one"]["calls"] >= 6
    return True

assert asyncio.run(test())
print("BENCHMARK_PASS")
""",
        scoring_hints="asyncio.Semaphore for concurrency, sync-to-async wrapping, per-stage stats, error callbacks, conditional stages",
    ),
]


# ======================================================================
# Scoring â€” evaluate generated code from the agent
# ======================================================================

@dataclass
class BenchmarkResult:
    task_id: str
    difficulty: str
    passed: bool
    score: float            # 0â€“10
    error: str = ""
    code_generated: str = ""
    time_taken: float = 0.0

    def to_dict(self) -> dict:
        return asdict(self)


def _extract_code_from_response(response: str) -> str:
    """Extract the largest Python code block from agent response."""
    import re
    blocks = re.findall(r"```python\s*\n(.*?)```", response, re.DOTALL)
    if blocks:
        # Return the longest block
        return max(blocks, key=len)
    # Fallback: try any code block
    blocks = re.findall(r"```\s*\n(.*?)```", response, re.DOTALL)
    if blocks:
        return max(blocks, key=len)
    return ""


def _run_validation(task: BenchmarkTask, generated_code: str) -> Tuple[bool, str]:
    """Execute the validation code with the generated code injected.
    Returns (passed, error_message).
    """
    full_code = task.validation_code.replace("{generated_code}", generated_code)

    try:
        result = subprocess.run(
            [sys.executable, "-c", full_code],
            capture_output=True, text=True, timeout=60,
            cwd=PROJECT_ROOT,
        )
        if result.returncode == 0 and "BENCHMARK_PASS" in result.stdout:
            return True, ""
        error = result.stderr.strip() or result.stdout.strip()
        return False, error[:1000]
    except subprocess.TimeoutExpired:
        return False, "Execution timed out (60s)"
    except Exception as e:
        return False, str(e)[:500]


def _clear_working_memory(agent: Any):
    """Clear per-task short context to avoid cross-task contamination."""
    try:
        if hasattr(agent, "memory") and hasattr(agent.memory, "working"):
            agent.memory.working.clear()
    except Exception as e:
        logger.debug(f"Working memory clear skipped: {e}")


def _build_validation_retry_prompt(task: BenchmarkTask, previous_code: str, validation_error: str, attempt: int) -> str:
    """Build a focused repair prompt using concrete validation feedback."""
    task_specific_fix = ""
    if task.id == "hard_calc_parser":
        err_lower = validation_error.lower()
        error_focused_fix = ""
        if "invalid literal for int()" in validation_error or "invalid number" in err_lower:
            error_focused_fix += "\n7) ä¿®å¤ç©ºæ•°å­—è§£æï¼š_read_number è¯»å–å‰å¿…é¡»ç¡®è®¤å½“å‰å­—ç¬¦æ˜¯æ•°å­—æˆ– '.'ï¼Œå¦åˆ™ä¸èƒ½è¿›å…¥ int/float è½¬æ¢"
        if "assertionerror" in err_lower:
            error_focused_fix += "\n8) ä¿®å¤æ–­è¨€å¤±è´¥ï¼šæ£€æŸ¥ evaluate æœ«å°¾æ˜¯å¦è¿˜æœ‰å‰©ä½™ tokenï¼Œå¹¶ç¡®ä¿è´Ÿå·/å‡å·è¯­ä¹‰æ­£ç¡®"

        task_specific_fix = (
            "\n4) Tokenizer å¤„ç†è´Ÿå·æ—¶ï¼Œä¸èƒ½æŠŠå•ç‹¬ '-' ä¼ å…¥æ•°å­—è§£æï¼›ä»…å½“ '-' åç´§è·Ÿæ•°å­—/å°æ•°ç‚¹æ—¶æ‹¼æˆè´Ÿæ•°\n"
            "5) æ”¯æŒå‡å·è¿ç®—ï¼ˆå¦‚ 2-1ï¼‰ä¸å¼€å¤´/æ‹¬å·åçš„è´Ÿå·ï¼ˆå¦‚ -5, (-3+1)ï¼‰\n"
            "6) evaluate è§£æåéœ€æ ¡éªŒæ— å‰©ä½™ tokenï¼Œé¿å…æ–­è¨€å¤±è´¥"
            f"{error_focused_fix}"
        )

    if task.id == "hard_concurrent_pool":
        err_lower = validation_error.lower()
        error_focused_fix = ""
        if "timeout" in err_lower:
            error_focused_fix += (
                "\n9) ä¿®å¤è¶…æ—¶ï¼šworker å¿…é¡»æŒç»­æ¶ˆè´¹é˜Ÿåˆ—å¹¶æ‰§è¡Œä»»åŠ¡ï¼Œ"
                "æ¯ä¸ªä»»åŠ¡å®Œæˆåè¦è®¾ç½® future ç»“æœ/å¼‚å¸¸ï¼Œé¿å… result(timeout) å¡æ­»"
            )
        if "not supported between instances" in err_lower or "priorityqueue" in err_lower:
            error_focused_fix += (
                "\n10) ä¿®å¤ PriorityQueue æ¯”è¾ƒé”™è¯¯ï¼šé˜Ÿåˆ—å…ƒç´ å¿…é¡»å§‹ç»ˆæ˜¯å¯æ¯”è¾ƒå…ƒç»„ï¼Œ"
                "ä½¿ç”¨ (priority, seq, ...) å¹¶ç¡®ä¿ seq é€’å¢ï¼›ä¸è¦è®© None ç›´æ¥å‚ä¸æ¯”è¾ƒ"
            )

        task_specific_fix = (
            "\n4) Future.result(timeout) å¿…é¡»è¿”å›ç»“æœï¼Œä»»åŠ¡å¤±è´¥æ—¶æŠ›å‡ºåŸå§‹å¼‚å¸¸ï¼›ä¸è¦åæ‰å¼‚å¸¸\n"
            "5) TaskPool.submit å¿…é¡»æŠŠä»»åŠ¡çœŸæ­£æ”¾å…¥é˜Ÿåˆ—å¹¶è¢« worker æ‰§è¡Œï¼Œä¸èƒ½åªåˆ›å»º Future\n"
            "6) PriorityQueue å…ƒç´ ç»Ÿä¸€ä¸º (priority, seq, payload) ç»“æ„ï¼Œpriority ç›¸åŒé  seq æ‰“ç ´å¹³å±€\n"
            "7) shutdown(wait=True) è¦å‘é€ä¸ worker æ•°é‡ç›¸åŒçš„åœæ­¢ä¿¡å·å¹¶ joinï¼Œé¿å…æŒ‚èµ·\n"
            "8) ä»…è¾“å‡ºä¸€ä¸ªå®Œæ•´ä»£ç å—ï¼Œä¸” TaskPool/Future å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰"
            f"{error_focused_fix}"
        )

    return (
        f"{task.prompt}\n\n"
        f"ä¸Šä¸€æ¬¡æäº¤æœªé€šè¿‡éªŒè¯ï¼ˆç¬¬{attempt}æ¬¡ä¿®å¤ï¼‰ã€‚è¯·æ ¹æ®é”™è¯¯ä¿®å¤ä»£ç å¹¶è¿”å›å®Œæ•´å®ç°ã€‚\n\n"
        f"éªŒè¯é”™è¯¯ï¼š\n{validation_error[:800]}\n\n"
        f"ä¸Šä¸€æ¬¡ä»£ç ï¼š\n```python\n{previous_code[:MAX_FEWSHOT_INJECT_CHARS]}\n```\n\n"
        "è¦æ±‚ï¼š\n"
        "1) è¾“å‡ºå®Œæ•´å¯è¿è¡Œä»£ç ï¼ˆä¸è¦çœç•¥ï¼‰\n"
        "2) ä¿ç•™é¢˜ç›®è¦æ±‚çš„é¡¶å±‚å‡½æ•°/ç±»ç­¾å\n"
        "3) ä»…è¾“å‡ºä¸€ä¸ª Python ä»£ç å—"
        f"{task_specific_fix}"
    )


def _retry_with_validation_feedback(agent: Any, task: BenchmarkTask, code: str, error: str, max_retries: int) -> Tuple[bool, str, str, int, float]:
    """Retry failed benchmark by feeding validation errors back to the agent."""
    if max_retries <= 0:
        return False, error, code, 0, 0.0

    retry_start = time.time()
    current_code = code
    current_error = error

    for attempt in range(1, max_retries + 1):
        _clear_working_memory(agent)
        prompt = _build_validation_retry_prompt(task, current_code, current_error, attempt)

        try:
            response = agent.chat(prompt)
        except Exception as e:
            current_error = f"retry agent error: {e}"
            continue

        new_code = _extract_code_from_response(response)
        if not new_code.strip():
            current_error = "retry produced no code"
            continue

        passed, new_error = _run_validation(task, new_code)
        current_code = new_code
        current_error = new_error
        if passed:
            return True, "", current_code, attempt, time.time() - retry_start

    return False, current_error, current_code, max_retries, time.time() - retry_start


def _score_quality(task: BenchmarkTask, code: str, passed: bool) -> float:
    """Score code quality on a 0â€“10 scale."""
    score = 0.0
    if not code.strip():
        return 0.0

    # correctness (most important)
    if passed:
        score += 5.0
    else:
        score += 1.0  # attempted, partial credit

    # type annotations
    if ": " in code and "->" in code:
        score += 1.0
    elif ": " in code:
        score += 0.5

    # docstring
    if '"""' in code or "'''" in code:
        score += 1.0

    # code length proportionality â€” reward non-trivial solutions
    lines = [l for l in code.split("\n") if l.strip() and not l.strip().startswith("#")]
    if 5 <= len(lines) <= 200:
        score += 0.5

    # error handling
    if "try" in code and "except" in code:
        score += 0.5

    # no forbidden constructs
    if "eval(" not in code and "exec(" not in code:
        score += 0.5

    # comments/clarity
    comment_lines = sum(1 for l in code.split("\n") if l.strip().startswith("#"))
    if comment_lines >= 2:
        score += 0.5

    return min(score, task.max_score)


# ======================================================================
# Evolution state â€” persistent tracking
# ======================================================================

# ======================================================================
# Acceleration constants
# ======================================================================

# Skip a task if it passed consecutively for this many rounds
SKIP_AFTER_CONSECUTIVE_PASSES = 3
# Do a full regression test every N rounds (even for passed tasks)
FULL_REGRESSION_INTERVAL = 5
# Skip reflection/improvement when pass rate exceeds this
SKIP_REFLECT_ABOVE_PASS_RATE = 1.0  # i.e. all tasks passed
# Use rule-based reflection (no LLM) when pass rate exceeds this
RULE_REFLECT_ABOVE_PASS_RATE = 0.875  # 7/8
# Retry failed tasks with explicit validation feedback
VALIDATION_FEEDBACK_RETRIES = 1
# Extra retries for tasks with higher historical volatility
TASK_VALIDATION_RETRY_OVERRIDES: Dict[str, int] = {
    "med_decorator_retry": 2,
    "hard_calc_parser": 3,
    "hard_concurrent_pool": 3,
}
# Keep fuller best-code context for hard tasks
MAX_BEST_CODE_CHARS = 12000
MAX_FEWSHOT_INJECT_CHARS = 6000


@dataclass
class EvolutionState:
    """Tracks progress across evolution rounds."""
    round_number: int = 0
    total_benchmarks_run: int = 0
    total_passed: int = 0
    best_score: float = 0.0
    best_round: int = 0
    improvements_applied: int = 0
    improvements_failed: int = 0
    rounds: List[Dict[str, Any]] = field(default_factory=list)
    started_at: float = field(default_factory=time.time)
    # Per-task consecutive pass streak: {task_id: consecutive_pass_count}
    task_pass_streaks: Dict[str, int] = field(default_factory=dict)
    # Best code for each task (for few-shot injection)
    task_best_code: Dict[str, str] = field(default_factory=dict)

    @property
    def pass_rate(self) -> float:
        return self.total_passed / self.total_benchmarks_run if self.total_benchmarks_run else 0.0

    def to_dict(self) -> dict:
        d = asdict(self)
        d["pass_rate"] = self.pass_rate
        d["elapsed_minutes"] = (time.time() - self.started_at) / 60
        return d

    def save(self, path: str):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)

    def update_task_streak(self, task_id: str, passed: bool, code: str = ""):
        """Update consecutive pass streak for a task."""
        if passed:
            self.task_pass_streaks[task_id] = self.task_pass_streaks.get(task_id, 0) + 1
            # Store best code (highest streak = most reliable solution)
            if code and self.task_pass_streaks[task_id] >= 2:
                self.task_best_code[task_id] = code[:MAX_BEST_CODE_CHARS]
        else:
            self.task_pass_streaks[task_id] = 0

    def should_skip_task(self, task_id: str, round_num: int) -> bool:
        """Whether a task can be skipped this round."""
        # Full regression every N rounds
        if round_num % FULL_REGRESSION_INTERVAL == 0:
            return False
        streak = self.task_pass_streaks.get(task_id, 0)
        return streak >= SKIP_AFTER_CONSECUTIVE_PASSES

    @classmethod
    def load(cls, path: str) -> "EvolutionState":
        if not os.path.exists(path):
            return cls()
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            state = cls()
            state.round_number = data.get("round_number", 0)
            state.total_benchmarks_run = data.get("total_benchmarks_run", 0)
            state.total_passed = data.get("total_passed", 0)
            state.best_score = data.get("best_score", 0.0)
            state.best_round = data.get("best_round", 0)
            state.improvements_applied = data.get("improvements_applied", 0)
            state.improvements_failed = data.get("improvements_failed", 0)
            state.rounds = data.get("rounds", [])
            state.started_at = data.get("started_at", time.time())
            state.task_pass_streaks = data.get("task_pass_streaks", {})
            state.task_best_code = data.get("task_best_code", {})
            return state
        except Exception as e:
            logger.error(f"Failed to load evolution state: {e}")
            return cls()


STATE_PATH = os.path.join(LOG_DIR, "evolution_state.json")


# ======================================================================
# Test suite runner
# ======================================================================

def run_test_suite() -> Tuple[bool, str]:
    """Run all 3 test suites. Returns (all_passed, output)."""
    test_files = ["test_agent.py", "test_memory_agent.py", "test_reflection_agent.py"]
    all_passed = True
    outputs = []

    for tf in test_files:
        path = os.path.join(PROJECT_ROOT, tf)
        if not os.path.exists(path):
            outputs.append(f"  {tf}: MISSING")
            all_passed = False
            continue
        try:
            result = subprocess.run(
                [sys.executable, path],
                capture_output=True, text=True, timeout=60,
                cwd=PROJECT_ROOT,
            )
            if result.returncode == 0 and "PASS" in result.stdout:
                outputs.append(f"  {tf}: PASS")
            else:
                outputs.append(f"  {tf}: FAIL â€” {result.stderr[:200]}")
                all_passed = False
        except subprocess.TimeoutExpired:
            outputs.append(f"  {tf}: TIMEOUT")
            all_passed = False
        except Exception as e:
            outputs.append(f"  {tf}: ERROR â€” {e}")
            all_passed = False

    return all_passed, "\n".join(outputs)


# ======================================================================
# Main evolution loop
# ======================================================================

def evolve(max_rounds: int = 20, resume: bool = False):
    """Run the autonomous evolution loop."""

    # Load or create state
    state = EvolutionState.load(STATE_PATH) if resume else EvolutionState()

    # Import and create agent
    from agent.core import CodingAgent, create_agent
    logger.info("=" * 70)
    logger.info("PyCoder Autonomous Evolution â€” Starting")
    logger.info(f"Max rounds: {max_rounds}, Benchmark tasks: {len(BENCHMARK_TASKS)}")
    logger.info("=" * 70)

    agent = create_agent()
    _ = agent.model  # trigger model load
    logger.info(f"Agent loaded: {agent.status()}")

    # Pre-flight test
    logger.info("Pre-flight: running test suite...")
    tests_ok, test_out = run_test_suite()
    logger.info(f"Test suite:\n{test_out}")
    if not tests_ok:
        logger.error("Pre-flight test suite FAILED â€” fix tests before evolving")
        return

    start_round = state.round_number + 1

    for round_num in range(start_round, start_round + max_rounds):
        state.round_number = round_num
        round_start = time.time()

        logger.info("")
        logger.info("=" * 70)
        logger.info(f"EVOLUTION ROUND {round_num}")
        logger.info("=" * 70)

        # ---- Phase 1: Benchmark ----
        is_full_regression = (round_num % FULL_REGRESSION_INTERVAL == 0)
        skipped_tasks: List[BenchmarkResult] = []  # tasks skipped due to streak
        active_tasks: List[BenchmarkTask] = []

        for task in BENCHMARK_TASKS:
            if state.should_skip_task(task.id, round_num):
                streak = state.task_pass_streaks.get(task.id, 0)
                skipped_tasks.append(BenchmarkResult(
                    task_id=task.id, difficulty=task.difficulty,
                    passed=True, score=8.5, error="",  # assume pass
                    code_generated="(skipped â€” streak={0})".format(streak),
                ))
            else:
                active_tasks.append(task)

        if skipped_tasks:
            logger.info(f"Phase 1: Running benchmarks... ({len(active_tasks)} active, {len(skipped_tasks)} skipped)")
        else:
            logger.info("Phase 1: Running benchmarks...")

        round_results: List[BenchmarkResult] = list(skipped_tasks)
        round_passed = len(skipped_tasks)  # skipped tasks count as passed
        round_total_score = sum(r.score for r in skipped_tasks)

        # Collect (task, code) pairs for parallel validation
        pending_validations: List[Tuple[BenchmarkTask, str, float]] = []

        for task in active_tasks:
            logger.info(f"  Benchmark [{task.difficulty}] {task.id}...")
            t0 = time.time()

            # Build prompt â€” inject few-shot hint for previously-failed tasks
            prompt = task.prompt
            streak = state.task_pass_streaks.get(task.id, 0)
            best_code = state.task_best_code.get(task.id, "")
            if streak == 0 and best_code:
                # Few-shot: show a previously successful solution as reference
                prompt = (
                    f"{task.prompt}\n\n"
                    f"å‚è€ƒï¼šä»¥ä¸‹æ˜¯ä¹‹å‰ä¸€ä¸ªé€šè¿‡éªŒè¯çš„å®ç°ï¼ˆä»…ä¾›å‚è€ƒï¼Œè¯·åœ¨æ­¤åŸºç¡€ä¸Šæ”¹è¿›ï¼‰ï¼š\n"
                    f"```python\n{best_code[:MAX_FEWSHOT_INJECT_CHARS]}\n```"
                )

            if task.id == "med_decorator_retry":
                prompt += (
                    "\n\né¢å¤–çº¦æŸï¼š\n"
                    "- å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ auto_retry\n"
                    "- åŒæ­¥ wrapper ä¸å¼‚æ­¥ async wrapper éƒ½è¦å®ç°\n"
                    "- ä¸è¦è¾“å‡ºè§£é‡Šï¼Œä¸è¦çœç•¥ä»£ç "
                )

            if task.id == "hard_calc_parser":
                prompt += (
                    "\n\né¢å¤–çº¦æŸï¼š\n"
                    "- Tokenizer ä¸­è´Ÿå·ä»…åœ¨è¡¨è¾¾å¼å¼€å¤´æˆ– '(' åï¼Œä¸”åé¢æ˜¯æ•°å­—/å°æ•°ç‚¹æ—¶æ‹¼å…¥æ•°å­—\n"
                    "- å…¶ä»– '-' å¿…é¡»ä½œä¸ºå‡æ³•è¿ç®—ç¬¦ tokenï¼Œè€Œä¸æ˜¯æ•°å­—çš„ä¸€éƒ¨åˆ†\n"
                    "- Parser éœ€æ­£ç¡®å®ç°ä¼˜å…ˆçº§ï¼šexpression(+/-) > term(*//) > factor(æ‹¬å·/æ•°å­—/ä¸€å…ƒè´Ÿå·)\n"
                    "- evaluate å¿…é¡»æ ¡éªŒè§£æå®Œæˆåæ— å‰©ä½™ token\n"
                    "- æäº¤å‰è¯·è‡ªæ£€ç¤ºä¾‹ï¼š'-5+3'ã€'2-1'ã€'2*(-3+1)'ã€'3.14*2'"
                )

            if task.id == "hard_concurrent_pool":
                prompt += (
                    "\n\né¢å¤–çº¦æŸï¼š\n"
                    "- å¿…é¡»åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰ TaskPool å’Œ Future\n"
                    "- Future.result(timeout) éœ€è¦ç­‰å¾…å¹¶åœ¨å¤±è´¥æ—¶æŠ›å‡ºåŸå§‹å¼‚å¸¸\n"
                    "- submit åä»»åŠ¡å¿…é¡»è¢« worker å®é™…æ‰§è¡Œå¹¶æœ€ç»ˆå®Œæˆ futureï¼ˆresult æˆ– exceptionï¼‰\n"
                    "- shutdown(wait=True) åä¸èƒ½å†å¡ä½ä¸»çº¿ç¨‹ï¼Œéœ€æ­£ç¡®å‘é€åœæ­¢ä¿¡å·å¹¶ join worker\n"
                    "- PriorityQueue å…ƒç´ å¿…é¡»ç»Ÿä¸€ä¸ºå¯æ¯”è¾ƒå…ƒç»„(å»ºè®®: priority, seq, payload)ï¼Œé¿å… None/object æ¯”è¾ƒå¼‚å¸¸\n"
                    "- æäº¤å‰è¯·è‡ªæ£€ï¼šsubmit(lambda x:x*2,21) èƒ½åœ¨ 5s å†…è¿”å› 42ï¼›fail() å¼‚å¸¸å¯è¢« future.result é€ä¼ "
                )

            # Ask the agent to solve the task
            try:
                _clear_working_memory(agent)
                response = agent.chat(prompt)
            except Exception as e:
                logger.error(f"    Agent error: {e}")
                round_results.append(BenchmarkResult(
                    task_id=task.id, difficulty=task.difficulty,
                    passed=False, score=0.0, error=str(e),
                ))
                state.update_task_streak(task.id, False)
                continue

            # Extract code from response
            code = _extract_code_from_response(response)
            if not code:
                logger.warning(f"    No code extracted from response")
                round_results.append(BenchmarkResult(
                    task_id=task.id, difficulty=task.difficulty,
                    passed=False, score=0.0, error="No code in response",
                    code_generated="",
                ))
                state.update_task_streak(task.id, False)
                continue

            pending_validations.append((task, code, t0))

        # ---- Parallel validation of all extracted code ----
        def _validate_one(item: Tuple[BenchmarkTask, str, float]) -> BenchmarkResult:
            task, code, t0 = item
            passed, error = _run_validation(task, code)
            score = _score_quality(task, code, passed)
            elapsed = time.time() - t0
            return BenchmarkResult(
                task_id=task.id, difficulty=task.difficulty,
                passed=passed, score=score, error=error,
                code_generated=code[:2000], time_taken=elapsed,
            )

        if pending_validations:
            with ThreadPoolExecutor(max_workers=min(len(pending_validations), 4)) as pool:
                futures = {pool.submit(_validate_one, item): item for item in pending_validations}
                for future in as_completed(futures):
                    result = future.result()
                    task_item = futures[future]
                    task_obj, code, _ = task_item

                    task_retry_count = TASK_VALIDATION_RETRY_OVERRIDES.get(task_obj.id, VALIDATION_FEEDBACK_RETRIES)
                    if not result.passed and task_retry_count > 0:
                        passed, final_error, final_code, retry_count, retry_elapsed = _retry_with_validation_feedback(
                            agent=agent,
                            task=task_obj,
                            code=code,
                            error=result.error,
                            max_retries=task_retry_count,
                        )
                        if retry_count > 0:
                            result.time_taken += retry_elapsed

                        if passed:
                            result.passed = True
                            result.error = ""
                            result.score = _score_quality(task_obj, final_code, True)
                            result.code_generated = final_code[:2000]
                            code = final_code
                            logger.info(f"    â†º RETRY PASS [{task_obj.id}] after {retry_count} feedback attempt(s)")
                        elif final_error:
                            result.error = final_error[:1000]
                            result.code_generated = final_code[:2000]

                    round_results.append(result)

                    state.update_task_streak(task_obj.id, result.passed, code)

                    if result.passed:
                        round_passed += 1
                        logger.info(f"    âœ“ PASS  [{task_obj.id}] score={result.score:.1f}  ({result.time_taken:.1f}s)")
                    else:
                        logger.info(f"    âœ— FAIL  [{task_obj.id}] score={result.score:.1f}  error={result.error[:100]}")

                    round_total_score += result.score

        total_tasks = len(BENCHMARK_TASKS)
        active_count = len(active_tasks)
        avg_score = round_total_score / total_tasks if total_tasks else 0.0
        current_pass_rate = round_passed / total_tasks if total_tasks else 0.0

        state.total_benchmarks_run += active_count  # only count actually-run tasks
        state.total_passed += round_passed - len(skipped_tasks)  # only count actually-run passes

        skip_info = f" (skipped {len(skipped_tasks)})" if skipped_tasks else ""
        logger.info(f"\nRound {round_num} benchmarks: {round_passed}/{total_tasks} passed{skip_info}, avg_score={avg_score:.1f}")

        # ---- Phase 2: Reflect on benchmark results ----
        failed_tasks = [r for r in round_results if not r.passed]
        passed_tasks = [r for r in round_results if r.passed]

        # Decide reflection strategy based on pass rate
        if current_pass_rate >= SKIP_REFLECT_ABOVE_PASS_RATE and not failed_tasks:
            # All passed â†’ skip LLM reflection entirely
            logger.info("Phase 2: All passed â€” skipping LLM reflection")
            logger.info("Phase 3: All passed â€” skipping self-improvement")
        elif current_pass_rate >= RULE_REFLECT_ABOVE_PASS_RATE:
            # High pass rate â†’ lightweight rule-based reflection (no LLM)
            logger.info("Phase 2: High pass rate â€” using rule-based reflection (no LLM)")
            if failed_tasks:
                for r in failed_tasks:
                    logger.info(f"  Weakness: [{r.difficulty}] {r.task_id} â€” {r.error[:150]}")

            # Only do self-improvement for actual failures
            if failed_tasks:
                logger.info(f"Phase 3: Targeted improvement for {len(failed_tasks)} failures...")
                try:
                    records = agent.improver.run_improvement_cycle()
                    if records:
                        applied = sum(1 for r in records if getattr(r, 'applied', False))
                        state.improvements_applied += applied
                        logger.info(f"  Self-improvement: {applied}/{len(records)} improvements applied")
                except Exception as e:
                    logger.error(f"Self-improvement cycle failed: {e}")
            else:
                logger.info("Phase 3: No failures â€” skipping")
        else:
            # Normal mode: full LLM reflection + improvement
            logger.info("Phase 2: Reflecting on results...")

            reflection_parts = [
                f"æˆ‘åˆšå®Œæˆç¬¬{round_num}è½®è‡ªæˆ‘æ¼”åŒ–çš„ç¼–ç¨‹åŸºå‡†æµ‹è¯•ã€‚",
                f"é€šè¿‡ç‡: {round_passed}/{total_tasks}",
                f"å¹³å‡å¾—åˆ†: {avg_score:.1f}/10",
            ]

            if failed_tasks:
                reflection_parts.append("\næœªé€šè¿‡çš„ä»»åŠ¡:")
                for r in failed_tasks:
                    reflection_parts.append(f"  - [{r.difficulty}] {r.task_id}: {r.error[:200]}")

            if passed_tasks:
                reflection_parts.append("\nå·²é€šè¿‡çš„ä»»åŠ¡:")
                for r in passed_tasks:
                    reflection_parts.append(f"  - [{r.difficulty}] {r.task_id}: score={r.score:.1f}")

            reflection_parts.append("\nè¯·è¿›è¡Œä¼šè¯åæ€å›é¡¾ï¼Œåˆ†ææˆ‘çš„å¼±ç‚¹å’Œéœ€è¦æ”¹è¿›çš„æ–¹å‘ã€‚")

            try:
                reflection_response = agent.chat("\n".join(reflection_parts))
                logger.info(f"Reflection:\n{reflection_response[:500]}")
            except Exception as e:
                logger.error(f"Reflection failed: {e}")

            # ---- Phase 3: Self-improvement ----
            if failed_tasks:
                logger.info(f"Phase 3: Self-improvement (targeting {len(failed_tasks)} weaknesses)...")

                improve_prompt = (
                    f"æ ¹æ®åŸºå‡†æµ‹è¯•ç»“æœï¼Œæˆ‘æœ‰ {len(failed_tasks)} ä¸ªä»»åŠ¡æœªé€šè¿‡ã€‚"
                    f"ä¸»è¦é—®é¢˜é›†ä¸­åœ¨: {', '.join(set(r.difficulty for r in failed_tasks))} çº§åˆ«ã€‚"
                    f"\nå¤±è´¥åŸå› æ€»ç»“:\n"
                )
                for r in failed_tasks:
                    improve_prompt += f"  - {r.task_id}: {r.error[:150]}\n"
                improve_prompt += "\nè¯·åˆ†æå¤±è´¥åŸå› å¹¶æå‡ºæ”¹è¿›æ–¹å‘ã€‚"

                try:
                    improve_response = agent.chat(improve_prompt)
                    logger.info(f"Reflection on failures:\n{improve_response[:500]}")
                except Exception as e:
                    logger.error(f"Failure reflection error: {e}")

                try:
                    logger.info("  Running self-improvement cycle...")
                    records = agent.improver.run_improvement_cycle()
                    if records:
                        applied = sum(1 for r in records if getattr(r, 'applied', False))
                        state.improvements_applied += applied
                        logger.info(f"  Self-improvement: {applied}/{len(records)} improvements applied")
                    else:
                        logger.info("  No improvements applied this round")
                except Exception as e:
                    logger.error(f"Self-improvement cycle failed: {e}")
                    logger.info("  No improvements applied this round")
            else:
                logger.info("Phase 3: All benchmarks passed â€” optimizing further...")
                try:
                    records = agent.improver.run_improvement_cycle()
                    if records:
                        applied = sum(1 for r in records if getattr(r, 'applied', False))
                        state.improvements_applied += applied
                        logger.info(f"  Optimization: {applied} improvements applied")
                except Exception as e:
                    logger.error(f"Optimization failed: {e}")

        # ---- Phase 4: Validate â€” test suite must still pass ----
        logger.info("Phase 4: Validating test suite...")
        tests_ok, test_out = run_test_suite()
        logger.info(f"Test suite:\n{test_out}")

        if not tests_ok:
            logger.warning("âš  Test suite FAILED after improvements - agent may have broken something")
            state.improvements_failed += 1
            # Give agent a chance to fix
            try:
                fix_response = agent.chat(
                    "æµ‹è¯•å¥—ä»¶åœ¨è‡ªæˆ‘æ”¹è¿›åå¤±è´¥äº†ï¼è¯·æ£€æŸ¥æœ€è¿‘çš„æ”¹åŠ¨å¹¶ä¿®å¤é—®é¢˜ã€‚"
                    f"\næµ‹è¯•è¾“å‡º:\n{test_out}"
                )
                logger.info(f"Fix attempt:\n{fix_response[:300]}")
                # Re-run tests
                tests_ok2, test_out2 = run_test_suite()
                if tests_ok2:
                    logger.info("  âœ“ Tests fixed")
                else:
                    logger.error("  âœ— Tests still failing")
            except Exception as e:
                logger.error(f"Fix attempt failed: {e}")

        # ---- Phase 5: Record round results ----
        round_data = {
            "round": round_num,
            "timestamp": time.time(),
            "passed": round_passed,
            "total": total_tasks,
            "avg_score": round(avg_score, 2),
            "pass_rate": round(round_passed / total_tasks, 3) if total_tasks else 0.0,
            "tests_ok": tests_ok,
            "results": [r.to_dict() for r in round_results],
            "elapsed_seconds": round(time.time() - round_start, 1),
        }
        state.rounds.append(round_data)

        if avg_score > state.best_score:
            state.best_score = avg_score
            state.best_round = round_num

        state.save(STATE_PATH)

        # ---- Progress report ----
        elapsed_total = (time.time() - state.started_at) / 60
        logger.info("")
        logger.info("-" * 50)
        logger.info(f"Round {round_num} complete:")
        logger.info(f"  Passed: {round_passed}/{total_tasks} ({round_passed/total_tasks*100:.0f}%)")
        logger.info(f"  Avg Score: {avg_score:.1f}/10")
        logger.info(f"  Best: {state.best_score:.1f}/10 (round {state.best_round})")
        logger.info(f"  Overall pass rate: {state.pass_rate*100:.1f}%")
        logger.info(f"  Improvements: +{state.improvements_applied} / -{state.improvements_failed}")
        logger.info(f"  Elapsed: {elapsed_total:.1f} min")
        logger.info("-" * 50)

        # ---- Early victory check ----
        if round_passed == total_tasks and avg_score >= 9.0:
            logger.info("ğŸ‰ ALL BENCHMARKS PASSED with high scores! Evolution target reached!")
            break

        # Save session periodically
        try:
            agent.save_session()
        except Exception as e:
            logger.error(f"Failed to save session: {e}")

    # ---- Final report ----
    logger.info("")
    logger.info("=" * 70)
    logger.info("EVOLUTION COMPLETE â€” FINAL REPORT")
    logger.info("=" * 70)
    logger.info(f"Rounds completed: {state.round_number}")
    logger.info(f"Total benchmarks run: {state.total_benchmarks_run}")
    logger.info(f"Total passed: {state.total_passed}")
    logger.info(f"Overall pass rate: {state.pass_rate*100:.1f}%")
    logger.info(f"Best avg score: {state.best_score:.1f}/10 (round {state.best_round})")
    logger.info(f"Improvements applied: {state.improvements_applied}")
    logger.info(f"Improvements failed: {state.improvements_failed}")
    logger.info(f"Total time: {(time.time()-state.started_at)/60:.1f} minutes")
    logger.info("=" * 70)

    agent.save_session()
    state.save(STATE_PATH)


# ======================================================================
# Entry point
# ======================================================================

def main():
    parser = argparse.ArgumentParser(
        description="PyCoder Autonomous Evolution Loop",
    )
    parser.add_argument(
        "--rounds", "-r", type=int, default=20,
        help="Maximum evolution rounds (default: 20)",
    )
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from last checkpoint",
    )
    args = parser.parse_args()

    evolve(max_rounds=args.rounds, resume=args.resume)


if __name__ == "__main__":
    main()
