#!/usr/bin/env python3
"""ç”Ÿæˆæ¯æ—¥è¿è¡ŒæŠ¥å‘Šï¼ˆMarkdownï¼‰ã€‚

èšåˆæ¥æºï¼š
- scripts/health_check.py
- scripts/window_report.py
- data/evolution/evolution_state.jsonï¼ˆè‹¥å­˜åœ¨ï¼‰
"""

from __future__ import annotations

import argparse
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple

PROJECT_ROOT = Path(__file__).resolve().parents[1]
REPORT_DIR = PROJECT_ROOT / "data" / "reports"


def run_command(cmd: list[str]) -> Tuple[int, str, str]:
    result = subprocess.run(
        cmd,
        cwd=PROJECT_ROOT,
        capture_output=True,
        text=True,
    )
    return result.returncode, result.stdout.strip(), result.stderr.strip()


def run_health_check(python_exec: str) -> Dict[str, Any]:
    code, out, err = run_command([python_exec, "scripts/health_check.py"])
    lines = [line for line in out.splitlines() if line.strip()]
    ok_count = sum(1 for x in lines if x.startswith("[OK]"))
    fail_count = sum(1 for x in lines if x.startswith("[FAIL]"))
    warn_count = sum(1 for x in lines if x.startswith("[WARN]"))
    return {
        "exit_code": code,
        "ok_count": ok_count,
        "fail_count": fail_count,
        "warn_count": warn_count,
        "output": out,
        "stderr": err,
        "passed": code == 0,
    }


def run_window_report(python_exec: str, window: int, compare_prev: bool) -> Dict[str, Any]:
    cmd = [python_exec, "scripts/window_report.py", "--window", str(window)]
    if compare_prev:
        cmd.extend([
            "--compare-prev",
            "--key-tasks",
            "hard_calc_parser",
            "hard_concurrent_pool",
            "med_decorator_retry",
        ])

    code, out, err = run_command(cmd)
    if code != 0:
        return {
            "ok": False,
            "error": f"window_report failed (code={code}): {err or out}",
        }

    try:
        payload = json.loads(out)
    except Exception as exc:
        return {
            "ok": False,
            "error": f"window_report json parse failed: {exc}",
            "raw": out,
        }

    return {
        "ok": True,
        "payload": payload,
    }


def load_state_snapshot() -> Dict[str, Any]:
    state_path = PROJECT_ROOT / "data" / "evolution" / "evolution_state.json"
    if not state_path.exists():
        return {}
    try:
        return json.loads(state_path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _fmt(v: Any) -> str:
    if isinstance(v, float):
        return f"{v:.4f}"
    return str(v)


def _extract_hard_task_rates(window_result: Dict[str, Any]) -> Tuple[float, float]:
    """Return (parser_rate, pool_rate), -1 when unavailable."""
    if not window_result.get("ok"):
        return -1.0, -1.0

    payload = window_result.get("payload", {})
    key_tasks = payload.get("key_tasks", {})

    parser_rate = -1.0
    pool_rate = -1.0

    parser_item = key_tasks.get("hard_calc_parser")
    if isinstance(parser_item, dict):
        if "new" in parser_item and isinstance(parser_item["new"], dict):
            parser_rate = float(parser_item["new"].get("pass_rate", -1.0))
        else:
            parser_rate = float(parser_item.get("pass_rate", -1.0))

    pool_item = key_tasks.get("hard_concurrent_pool")
    if isinstance(pool_item, dict):
        if "new" in pool_item and isinstance(pool_item["new"], dict):
            pool_rate = float(pool_item["new"].get("pass_rate", -1.0))
        else:
            pool_rate = float(pool_item.get("pass_rate", -1.0))

    # fallback: from current.tasks
    if parser_rate < 0 or pool_rate < 0:
        tasks = payload.get("current", {}).get("tasks", {})
        if parser_rate < 0 and "hard_calc_parser" in tasks:
            parser_rate = float(tasks["hard_calc_parser"].get("pass_rate", -1.0))
        if pool_rate < 0 and "hard_concurrent_pool" in tasks:
            pool_rate = float(tasks["hard_concurrent_pool"].get("pass_rate", -1.0))

    return parser_rate, pool_rate


def evaluate_quality_gate(
    health: Dict[str, Any],
    window_result: Dict[str, Any],
    avg_min: float,
    std_max: float,
    hard_min: float,
) -> Dict[str, Any]:
    """Evaluate daily quality gate and return detailed result."""
    reasons = []
    checks = {}

    health_ok = bool(health.get("passed"))
    checks["health_passed"] = health_ok
    if not health_ok:
        reasons.append("å¥åº·æ£€æŸ¥æœªé€šè¿‡")

    if not window_result.get("ok"):
        checks["window_report_ok"] = False
        reasons.append("çª—å£æŒ‡æ ‡ç”Ÿæˆå¤±è´¥")
        return {
            "passed": False,
            "checks": checks,
            "reasons": reasons,
            "current_avg": None,
            "current_std": None,
            "parser_rate": None,
            "pool_rate": None,
            "thresholds": {
                "avg_min": avg_min,
                "std_max": std_max,
                "hard_min": hard_min,
            },
        }

    checks["window_report_ok"] = True
    payload = window_result.get("payload", {})
    current = payload.get("current", {}).get("overall", {})
    current_avg = current.get("avg_round_pass_ratio")
    current_std = current.get("std_round_pass_ratio")

    avg_ok = isinstance(current_avg, (int, float)) and float(current_avg) >= avg_min
    std_ok = isinstance(current_std, (int, float)) and float(current_std) <= std_max
    checks["avg_pass_ratio"] = avg_ok
    checks["std_pass_ratio"] = std_ok

    if not avg_ok:
        reasons.append(f"avg_round_pass_ratio ä½ŽäºŽé˜ˆå€¼: {current_avg} < {avg_min}")
    if not std_ok:
        reasons.append(f"std_round_pass_ratio é«˜äºŽé˜ˆå€¼: {current_std} > {std_max}")

    parser_rate, pool_rate = _extract_hard_task_rates(window_result)
    parser_ok = parser_rate >= hard_min
    pool_ok = pool_rate >= hard_min
    checks["hard_calc_parser"] = parser_ok
    checks["hard_concurrent_pool"] = pool_ok

    if not parser_ok:
        reasons.append(f"hard_calc_parser pass_rate ä½ŽäºŽé˜ˆå€¼: {parser_rate} < {hard_min}")
    if not pool_ok:
        reasons.append(f"hard_concurrent_pool pass_rate ä½ŽäºŽé˜ˆå€¼: {pool_rate} < {hard_min}")

    return {
        "passed": all(checks.values()),
        "checks": checks,
        "reasons": reasons,
        "current_avg": current_avg,
        "current_std": current_std,
        "parser_rate": parser_rate,
        "pool_rate": pool_rate,
        "thresholds": {
            "avg_min": avg_min,
            "std_max": std_max,
            "hard_min": hard_min,
        },
    }


def build_markdown(
    generated_at: str,
    health: Dict[str, Any],
    window_result: Dict[str, Any],
    state: Dict[str, Any],
    window: int,
    gate: Dict[str, Any],
) -> str:
    lines: list[str] = []
    lines.append("# PyCoder æ¯æ—¥æŠ¥å‘Š")
    lines.append("")
    lines.append(f"- ç”Ÿæˆæ—¶é—´: {generated_at}")
    lines.append(f"- é¡¹ç›®è·¯å¾„: {PROJECT_ROOT}")
    lines.append(f"- ç»Ÿè®¡çª—å£: æœ€è¿‘ {window} è½®")
    lines.append("")

    # Summary
    lines.append("## æ¦‚è§ˆ")
    lines.append("")
    health_status = "PASS" if health.get("passed") else "FAIL"
    lines.append(f"- å¥åº·æ£€æŸ¥: **{health_status}** (OK={health.get('ok_count', 0)}, FAIL={health.get('fail_count', 0)}, WARN={health.get('warn_count', 0)})")

    if state:
        lines.append(f"- å½“å‰è½®æ¬¡: {state.get('round_number', 'N/A')}")
        lines.append(f"- æ€»ä½“é€šè¿‡çŽ‡: {_fmt(state.get('pass_rate', 'N/A'))}")
        lines.append(f"- ç´¯è®¡é€šè¿‡/æ€»æ•°: {state.get('total_passed', 'N/A')}/{state.get('total_benchmarks_run', 'N/A')}")
        lines.append(f"- Best score: {state.get('best_score', 'N/A')} (round {state.get('best_round', 'N/A')})")
    lines.append("")

    # Window metrics
    lines.append("## çª—å£æŒ‡æ ‡")
    lines.append("")
    if not window_result.get("ok"):
        lines.append(f"- ç”Ÿæˆå¤±è´¥: {window_result.get('error', 'unknown error')}")
        if window_result.get("raw"):
            lines.append("")
            lines.append("```text")
            lines.append(window_result["raw"][:2000])
            lines.append("```")
    else:
        payload = window_result["payload"]
        current = payload.get("current", {}).get("overall", {})
        lines.append(f"- å½“å‰çª—å£: {current.get('window', 'N/A')} | rounds={current.get('rounds', 'N/A')} | avg={_fmt(current.get('avg_round_pass_ratio', 'N/A'))} | std={_fmt(current.get('std_round_pass_ratio', 'N/A'))} | full_pass={current.get('full_pass_rounds', 'N/A')}")

        if "previous" in payload:
            prev = payload.get("previous", {}).get("overall", {})
            lines.append(f"- ä¸Šä¸€çª—å£: {prev.get('window', 'N/A')} | rounds={prev.get('rounds', 'N/A')} | avg={_fmt(prev.get('avg_round_pass_ratio', 'N/A'))} | std={_fmt(prev.get('std_round_pass_ratio', 'N/A'))} | full_pass={prev.get('full_pass_rounds', 'N/A')}")
            if "both_hard_ge_80" in payload:
                lines.append(f"- åŒ hard ä»»åŠ¡ >=0.80: {payload.get('both_hard_ge_80')}")

        key_tasks = payload.get("key_tasks", {})
        if key_tasks:
            lines.append("")
            lines.append("### å…³é”®ä»»åŠ¡")
            for task, item in key_tasks.items():
                if "new" in item:
                    new = item.get("new", {})
                    lines.append(
                        f"- {task}: pass_rate={_fmt(new.get('pass_rate', 'N/A'))}, "
                        f"fails={new.get('fails', 'N/A')}, flips={new.get('flips', 'N/A')}, "
                        f"streak={new.get('longest_pass_streak', 'N/A')}, "
                        f"delta_pass_rate={_fmt(item.get('delta_pass_rate', 'N/A'))}"
                    )
                else:
                    lines.append(
                        f"- {task}: pass_rate={_fmt(item.get('pass_rate', 'N/A'))}, "
                        f"fails={item.get('fails', 'N/A')}, flips={item.get('flips', 'N/A')}, "
                        f"streak={item.get('longest_pass_streak', 'N/A')}"
                    )

    lines.append("")
    lines.append("## è´¨é‡é—¸é—¨")
    lines.append("")
    gate_status = "PASS" if gate.get("passed") else "FAIL"
    lines.append(f"- Gate ç»“æžœ: **{gate_status}**")
    t = gate.get("thresholds", {})
    lines.append(
        f"- é˜ˆå€¼: avg>={_fmt(t.get('avg_min', 'N/A'))}, "
        f"std<={_fmt(t.get('std_max', 'N/A'))}, "
        f"hard_task>={_fmt(t.get('hard_min', 'N/A'))}"
    )
    lines.append(
        f"- å½“å‰: avg={_fmt(gate.get('current_avg', 'N/A'))}, "
        f"std={_fmt(gate.get('current_std', 'N/A'))}, "
        f"parser={_fmt(gate.get('parser_rate', 'N/A'))}, "
        f"pool={_fmt(gate.get('pool_rate', 'N/A'))}"
    )
    if gate.get("reasons"):
        lines.append("- æœªé€šè¿‡åŽŸå› :")
        for reason in gate["reasons"]:
            lines.append(f"  - {reason}")

    lines.append("")
    lines.append("## å¥åº·æ£€æŸ¥æ˜Žç»†")
    lines.append("")
    lines.append("```text")
    lines.append(health.get("output", "(no output)")[:4000])
    lines.append("```")

    if health.get("stderr"):
        lines.append("")
        lines.append("```text")
        lines.append(health["stderr"][:2000])
        lines.append("```")

    lines.append("")
    lines.append("## åŽç»­å»ºè®®")
    lines.append("")
    if gate.get("passed"):
        lines.append("- å½“å‰çŠ¶æ€å¯ç»§ç»­æŒ‰æ—¢å®šèŠ‚å¥è¿è¡Œï¼Œå»ºè®®æ¯æ—¥ä¿ç•™ä¸€ä»½æŠ¥å‘Šå½’æ¡£ã€‚")
    else:
        lines.append("- å­˜åœ¨å¼‚å¸¸æˆ–å›žå½’ï¼Œè¯·ä¼˜å…ˆæŸ¥çœ‹ docs/OPERATIONS_TROUBLESHOOTING.md å¹¶æ‰§è¡Œå°æ­¥ä¿®å¤ã€‚")

    return "\n".join(lines) + "\n"


def build_alert_markdown(
    generated_at: str,
    report_path: Path,
    gate: Dict[str, Any],
    health: Dict[str, Any],
    window_result: Dict[str, Any],
) -> str:
    """Build concise alert markdown for failed quality gate."""
    lines: list[str] = []
    lines.append("# ðŸš¨ PyCoder è´¨é‡é—¸é—¨å‘Šè­¦")
    lines.append("")
    lines.append(f"- è§¦å‘æ—¶é—´: {generated_at}")
    lines.append(f"- å¯¹åº”æ—¥æŠ¥: {report_path}")
    lines.append(f"- å¥åº·æ£€æŸ¥é€šè¿‡: {health.get('passed')}")
    lines.append("")

    lines.append("## Gate çŠ¶æ€")
    lines.append("")
    lines.append("- ç»“æžœ: **FAIL**")
    t = gate.get("thresholds", {})
    lines.append(
        f"- é˜ˆå€¼: avg>={_fmt(t.get('avg_min', 'N/A'))}, "
        f"std<={_fmt(t.get('std_max', 'N/A'))}, "
        f"hard_task>={_fmt(t.get('hard_min', 'N/A'))}"
    )
    lines.append(
        f"- å½“å‰: avg={_fmt(gate.get('current_avg', 'N/A'))}, "
        f"std={_fmt(gate.get('current_std', 'N/A'))}, "
        f"parser={_fmt(gate.get('parser_rate', 'N/A'))}, "
        f"pool={_fmt(gate.get('pool_rate', 'N/A'))}"
    )
    lines.append("")

    reasons = gate.get("reasons", [])
    if reasons:
        lines.append("## å¤±è´¥åŽŸå› ")
        lines.append("")
        for reason in reasons:
            lines.append(f"- {reason}")
        lines.append("")

    top_regressions = _extract_top_regressions(window_result, top_n=3)
    if top_regressions:
        lines.append("## Top 3 å›žå½’ä»»åŠ¡")
        lines.append("")
        for item in top_regressions:
            lines.append(
                f"- {item['task']}: pass_rate={_fmt(item.get('pass_rate'))}, "
                f"delta_pass_rate={_fmt(item.get('delta_pass_rate'))}, "
                f"fails={item.get('fails')}, flips={item.get('flips')}"
            )
        lines.append("")

    lines.append("## å»ºè®®åŠ¨ä½œ")
    lines.append("")
    lines.append("- æ£€æŸ¥ docs/OPERATIONS_TROUBLESHOOTING.md çš„å¯¹åº”æ•…éšœç« èŠ‚ã€‚")
    for action in _suggest_actions(top_regressions):
        lines.append(f"- {action}")
    lines.append("- ä¿®å¤åŽé‡è·‘ 8~10 è½®çª—å£å¹¶é‡æ–°ç”Ÿæˆæ—¥æŠ¥ã€‚")
    return "\n".join(lines) + "\n"


def _extract_top_regressions(window_result: Dict[str, Any], top_n: int = 3) -> list[Dict[str, Any]]:
    """Extract top regressions from compare payload; fallback to lowest pass-rate tasks."""
    if not window_result.get("ok"):
        return []

    payload = window_result.get("payload", {})
    key_tasks = payload.get("key_tasks", {})
    regressions: list[Dict[str, Any]] = []

    # Primary: use compare deltas when available
    for task, item in key_tasks.items():
        if isinstance(item, dict) and "new" in item:
            new_item = item.get("new", {})
            delta = float(item.get("delta_pass_rate", 0.0))
            regressions.append({
                "task": task,
                "pass_rate": float(new_item.get("pass_rate", -1.0)),
                "delta_pass_rate": delta,
                "fails": int(new_item.get("fails", -1)),
                "flips": int(new_item.get("flips", -1)),
                "score": delta,
            })

    if regressions:
        negatives = [r for r in regressions if r["delta_pass_rate"] < 0]
        if negatives:
            negatives.sort(key=lambda x: (x["delta_pass_rate"], x["pass_rate"]))
            return negatives[:top_n]

        # If no negative deltas, choose currently weakest tasks in compare set
        regressions.sort(key=lambda x: (x["pass_rate"], -x["fails"], x["flips"]))
        return regressions[:top_n]

    # Fallback: pick lowest current pass-rate tasks
    current_tasks = payload.get("current", {}).get("tasks", {})
    for task, item in current_tasks.items():
        regressions.append({
            "task": task,
            "pass_rate": float(item.get("pass_rate", -1.0)),
            "delta_pass_rate": 0.0,
            "fails": int(item.get("fails", -1)),
            "flips": int(item.get("flips", -1)),
            "score": float(item.get("pass_rate", -1.0)),
        })

    regressions.sort(key=lambda x: (x["score"], -x["fails"]))
    return regressions[:top_n]


def _suggest_actions(top_regressions: list[Dict[str, Any]]) -> list[str]:
    """Generate concise remediation hints based on top regressions."""
    if not top_regressions:
        return ["æœªè¯†åˆ«å…·ä½“å›žå½’ä»»åŠ¡ï¼Œå»ºè®®å…ˆæ£€æŸ¥ evolution.log æœ€è¿‘å¤±è´¥æ¡ç›®ã€‚"]

    action_map = {
        "hard_concurrent_pool": "hard_concurrent_poolï¼šä¼˜å…ˆæ£€æŸ¥ timeout ä¸Ž PriorityQueue å¯æ¯”è¾ƒå…ƒç»„è§„åˆ™ï¼Œç¡®è®¤ worker èƒ½å®Œæˆ futureã€‚",
        "hard_calc_parser": "hard_calc_parserï¼šä¼˜å…ˆæ£€æŸ¥è´Ÿå·è¯­ä¹‰ã€token æ¶ˆè€—å®Œæ•´æ€§ä¸Ž parse ç»“æŸæ— å‰©ä½™ tokenã€‚",
        "med_decorator_retry": "med_decorator_retryï¼šæ£€æŸ¥åŒæ­¥/å¼‚æ­¥é‡è¯•åˆ†æ”¯å’Œ max_retries è¯­ä¹‰ï¼ˆæ€»å°è¯•=1+max_retriesï¼‰ã€‚",
        "med_lru_cache": "med_lru_cacheï¼šæ£€æŸ¥ TTL è¿‡æœŸè·¯å¾„å’Œå¹¶å‘é”ç²’åº¦ï¼Œç¡®ä¿ get/put è¯­ä¹‰ä¸€è‡´ã€‚",
        "expert_async_pipeline": "expert_async_pipelineï¼šæ£€æŸ¥ semaphore å¹¶å‘æŽ§åˆ¶ä¸Žé˜¶æ®µç»Ÿè®¡æ˜¯å¦æ­£ç¡®ç´¯ç§¯ã€‚",
        "expert_type_checker": "expert_type_checkerï¼šæ£€æŸ¥æ³›åž‹å®¹å™¨ç±»åž‹åŒ¹é…é€»è¾‘ä¸Ž Union/Optional å¤„ç†ã€‚",
    }

    actions: list[str] = []
    seen = set()
    for item in top_regressions:
        task = item.get("task", "")
        if not task or task in seen:
            continue
        seen.add(task)
        actions.append(action_map.get(task, f"{task}ï¼šå…ˆå¤ç›˜æœ€è¿‘å¤±è´¥ tracebackï¼Œå†è¡¥å……ä»»åŠ¡å®šå‘æç¤ºä¸Žé‡è¯•è§„åˆ™ã€‚"))
    return actions


def main() -> int:
    parser = argparse.ArgumentParser(description="Generate daily markdown report")
    parser.add_argument("--window", type=int, default=10, help="Window size for metrics (default: 10)")
    parser.add_argument("--no-compare-prev", action="store_true", help="Disable previous-window comparison")
    parser.add_argument("--output", type=str, default="", help="Output markdown file path")
    parser.add_argument("--gate-avg-min", type=float, default=0.95, help="Quality gate minimum avg_round_pass_ratio")
    parser.add_argument("--gate-std-max", type=float, default=0.06, help="Quality gate maximum std_round_pass_ratio")
    parser.add_argument("--gate-hard-min", type=float, default=0.80, help="Quality gate minimum pass_rate for hard tasks")
    parser.add_argument("--fail-on-gate", action="store_true", help="Exit non-zero when quality gate fails")
    parser.add_argument("--alert-on-gate-fail", action="store_true", help="Write alert markdown when quality gate fails")
    parser.add_argument("--alert-output", type=str, default="data/reports/alert_latest.md", help="Alert markdown output path")
    args = parser.parse_args()

    REPORT_DIR.mkdir(parents=True, exist_ok=True)

    python_exec = sys.executable
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    health = run_health_check(python_exec)
    window_result = run_window_report(
        python_exec=python_exec,
        window=args.window,
        compare_prev=not args.no_compare_prev,
    )
    state = load_state_snapshot()
    gate = evaluate_quality_gate(
        health=health,
        window_result=window_result,
        avg_min=args.gate_avg_min,
        std_max=args.gate_std_max,
        hard_min=args.gate_hard_min,
    )

    md = build_markdown(
        generated_at=now,
        health=health,
        window_result=window_result,
        state=state,
        window=args.window,
        gate=gate,
    )

    output_path = Path(args.output) if args.output else (REPORT_DIR / f"daily_report_{stamp}.md")
    if not output_path.is_absolute():
        output_path = PROJECT_ROOT / output_path
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(md, encoding="utf-8")

    print(f"Report generated: {output_path}")

    if args.alert_on_gate_fail:
        alert_path = Path(args.alert_output)
        if not alert_path.is_absolute():
            alert_path = PROJECT_ROOT / alert_path
        alert_path.parent.mkdir(parents=True, exist_ok=True)

        if gate.get("passed"):
            if alert_path.exists():
                alert_path.unlink(missing_ok=True)
                print(f"Alert cleared: {alert_path}")
        else:
            alert_md = build_alert_markdown(
                generated_at=now,
                report_path=output_path,
                gate=gate,
                health=health,
                window_result=window_result,
            )
            alert_path.write_text(alert_md, encoding="utf-8")
            print(f"Alert generated: {alert_path}")

    if args.fail_on_gate and not gate.get("passed"):
        print("Quality gate failed.")
        return 2
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
